{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3652d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import logging\n",
    "import warnings\n",
    "DB_PATH='/opt/test-data/experimental.duckdb'\n",
    "\n",
    "# Use this line to ignore the specific UserWarning message\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    message='pandas only supports SQLAlchemy',\n",
    "    category=UserWarning\n",
    ")\n",
    "\n",
    "def load_sample_data(table_name, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Loads a sample of data from a DuckDB table into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): The name of the table to sample from.\n",
    "        sample_size (int): The number of rows to sample.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the sample data.\n",
    "    \"\"\"\n",
    "    # 1. Establish a connection to the DuckDB database file\n",
    "    # Using ':memory:' for a temporary in-memory database\n",
    "    # Replace with 'path/to/your/database.duckdb' to connect to a file\n",
    "    with duckdb.connect(database=DB_PATH, read_only=True) as conn:\n",
    "        try:\n",
    "            # 2. Use a SQL query to select a sample\n",
    "            query = f\"SELECT * FROM {table_name} using sample {sample_size} rows\"\n",
    "\n",
    "            # 3. Use pandas.read_sql to execute the query and load the data\n",
    "            df = pd.read_sql(query, conn)\n",
    "            \n",
    "            print(f\"Successfully loaded a sample of {len(df)} rows from {table_name}.\")\n",
    "            return df\n",
    "        \n",
    "        except duckdb.Error as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "def combine_query_results(queries, db_path=DB_PATH):\n",
    "    \"\"\"\n",
    "    Runs multiple SQL queries against a DuckDB database and combines the results\n",
    "    into a single pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): The path to the DuckDB database file.\n",
    "        queries (list): A list of SQL query strings.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single DataFrame containing the combined results.\n",
    "    \"\"\"\n",
    "    # List to hold the individual query results\n",
    "    result_dfs = []\n",
    "\n",
    "    # Connect to the DuckDB database\n",
    "    # Using a 'with' statement ensures the connection is closed automatically\n",
    "    with duckdb.connect(database=db_path, read_only=True) as conn:\n",
    "        print(\"Connected to DuckDB database.\")\n",
    "        for i, query in enumerate(queries):\n",
    "            try:\n",
    "                # Use pandas.read_sql to execute the query\n",
    "                print(f\"Executing query {i + 1}...\")\n",
    "                df = pd.read_sql(query, conn)\n",
    "                result_dfs.append(df)\n",
    "            except duckdb.Error as e:\n",
    "                print(f\"Error executing query {i + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Use pd.concat() to combine all DataFrames in the list\n",
    "    if result_dfs:\n",
    "        combined_df = pd.concat(result_dfs, ignore_index=True)\n",
    "        print(\"All query results combined successfully.\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No results to combine.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9341a02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded a sample of 20000 rows from er.pair_features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_id_a</th>\n",
       "      <th>company_id_b</th>\n",
       "      <th>domain_exact</th>\n",
       "      <th>country_exact</th>\n",
       "      <th>city_exact</th>\n",
       "      <th>name_exact</th>\n",
       "      <th>name_compact_exact</th>\n",
       "      <th>name_prefix4_eq</th>\n",
       "      <th>name_prefix6_eq</th>\n",
       "      <th>emp_cur_absdiff</th>\n",
       "      <th>emp_cur_reldiff</th>\n",
       "      <th>emp_tot_absdiff</th>\n",
       "      <th>emp_tot_reldiff</th>\n",
       "      <th>both_have_domain</th>\n",
       "      <th>both_have_name</th>\n",
       "      <th>country_a</th>\n",
       "      <th>country_b</th>\n",
       "      <th>city_a</th>\n",
       "      <th>city_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>808699</td>\n",
       "      <td>4691738</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>25</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>united states</td>\n",
       "      <td>united states</td>\n",
       "      <td>sacramento</td>\n",
       "      <td>coral gables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4149873</td>\n",
       "      <td>7131113</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>brazil</td>\n",
       "      <td>brazil</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>sao paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4240637</td>\n",
       "      <td>6429044</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>93</td>\n",
       "      <td>0.989362</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>united states</td>\n",
       "      <td>united states</td>\n",
       "      <td>state college</td>\n",
       "      <td>lititz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2103768</td>\n",
       "      <td>3403384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>india</td>\n",
       "      <td>india</td>\n",
       "      <td>pune</td>\n",
       "      <td>bombay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3046615</td>\n",
       "      <td>4102729</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>hampshire</td>\n",
       "      <td>swansea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company_id_a  company_id_b  domain_exact  country_exact  city_exact  \\\n",
       "0        808699       4691738             0              1         0.0   \n",
       "1       4149873       7131113             0              1         1.0   \n",
       "2       4240637       6429044             0              1         0.0   \n",
       "3       2103768       3403384             0              1         0.0   \n",
       "4       3046615       4102729             0              1         0.0   \n",
       "\n",
       "   name_exact  name_compact_exact  name_prefix4_eq  name_prefix6_eq  \\\n",
       "0           0                   0                1                1   \n",
       "1           0                   0                0                0   \n",
       "2           0                   0                1                0   \n",
       "3           0                   0                1                0   \n",
       "4           0                   0                1                0   \n",
       "\n",
       "   emp_cur_absdiff  emp_cur_reldiff  emp_tot_absdiff  emp_tot_reldiff  \\\n",
       "0               14         0.736842               25         0.757576   \n",
       "1                1         0.500000                1         0.500000   \n",
       "2               68         1.000000               93         0.989362   \n",
       "3                2         0.666667                2         0.666667   \n",
       "4                2         1.000000                1         0.333333   \n",
       "\n",
       "   both_have_domain  both_have_name       country_a       country_b  \\\n",
       "0                 1               1   united states   united states   \n",
       "1                 1               1          brazil          brazil   \n",
       "2                 1               1   united states   united states   \n",
       "3                 1               1           india           india   \n",
       "4                 1               1  united kingdom  united kingdom   \n",
       "\n",
       "          city_a        city_b  \n",
       "0     sacramento  coral gables  \n",
       "1      sao paulo     sao paulo  \n",
       "2  state college        lititz  \n",
       "3           pune        bombay  \n",
       "4      hampshire       swansea  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = load_sample_data(\"er.pair_features\",20000)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891be117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos size: 2007\n",
      "neg size: 1851\n",
      "size of weak_training set 3858\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.996     0.996     0.996       463\n",
      "           1      0.996     0.996     0.996       502\n",
      "\n",
      "    accuracy                          0.996       965\n",
      "   macro avg      0.996     0.996     0.996       965\n",
      "weighted avg      0.996     0.996     0.996       965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your pair_features (CSV or from DuckDB via .df())\n",
    "#df = pd.read_csv(\"pair_features_sample.csv\")\n",
    "# --- Weak labels ---\n",
    "pos_mask = (df[\"domain_exact\"] == 1) | (\n",
    "    (df[\"country_exact\"] == 1) &\n",
    "    (df[\"name_prefix6_eq\"] == 1) &\n",
    "    (df[\"emp_tot_reldiff\"].fillna(1.0) <= 0.20)\n",
    ")\n",
    "\n",
    "neg_mask = (df[\"country_exact\"] == 0) | (\n",
    "    (df[\"name_prefix4_eq\"] == 0) &\n",
    "    (df[\"emp_tot_reldiff\"].fillna(1.0) >= 0.90)\n",
    ")\n",
    "\n",
    "# remove overlaps: positives win ties\n",
    "neg_mask = neg_mask & (~pos_mask)\n",
    "\n",
    "pos = df[pos_mask].copy()\n",
    "pos[\"label\"] = 1\n",
    "neg = df[neg_mask].copy()\n",
    "neg[\"label\"] = 0\n",
    "\n",
    "print(f\"pos size: {len(pos)}\")\n",
    "print(f\"neg size: {len(neg)}\")\n",
    "# downsample negatives to balance\n",
    "# chances are the data will be mostly 'nonmatch' so \n",
    "# we want to try to offset that \n",
    "neg_sample = neg.sample(n=min(len(pos), len(neg)), random_state=42)\n",
    "weak_train = pd.concat([pos, neg_sample], ignore_index=True).sample(frac=1, random_state=42)\n",
    "print(f\"size of weak_training set {len(weak_train)}\")\n",
    "# --- Features / target ---\n",
    "drop_id_cols = [\"company_id_a\", \"company_id_b\"]\n",
    "X = weak_train.drop(columns=[\"label\"] + drop_id_cols, errors=\"ignore\")\n",
    "y = weak_train[\"label\"]\n",
    "\n",
    "# 2) Work out which categorical columns are actually present\n",
    "candidate_cat = [\"country_a\", \"country_b\", \"city_a\", \"city_b\"]\n",
    "cat_cols = [c for c in candidate_cat if c in X.columns]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "# (Optional) quick sanity: see if anything is missing\n",
    "missing = set(cat_cols) - set(X.columns)\n",
    "assert not missing, f\"These categorical columns are missing from X: {missing}\"\n",
    "\n",
    "# 3) Preprocess: impute NaNs, one-hot the cats, pass through nums\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", make_pipeline(SimpleImputer(strategy=\"median\")), num_cols),\n",
    "        (\"cat\", make_pipeline(SimpleImputer(strategy=\"most_frequent\"),\n",
    "                              OneHotEncoder(handle_unknown=\"ignore\")), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# 4) Model\n",
    "clf = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(classification_report(y_val, clf.predict(X_val), digits=3))\n",
    "\n",
    "# Probabilities for downstream use:\n",
    "val_proba = clf.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf869367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/er_pair_clf.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, \"../models/er_pair_clf.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09501c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to DuckDB database.\n",
      "Executing query 1...\n",
      "Executing query 2...\n",
      "Executing query 3...\n",
      "Executing query 4...\n",
      "All query results combined successfully.\n",
      "\n",
      "Final Combined DataFrame:\n",
      "   count_star()\n",
      "0     155577811\n",
      "1     155577811\n",
      "2     172867797\n",
      "3             0\n"
     ]
    }
   ],
   "source": [
    "sanity_checks=[\n",
    "  \"SELECT COUNT(*) FROM er.pair_features ;\",\n",
    "\"SELECT COUNT(*)  FROM er.pair_scores;\",\n",
    "\"SELECT COUNT(*) FROM er.pair_features_scored ;\",\n",
    "\"SELECT COUNT(*)  FROM er.pair_features_scored  WHERE model_score IS NULL;\",\n",
    "]\n",
    "final_df = combine_query_results(sanity_checks)\n",
    "print(\"\\nFinal Combined DataFrame:\")\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57543b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_pair_features,total_pair_scores,total_pair_features_scored,total_null_pair_features_scored\n",
      "155577811,155577811,155577811,0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "better_sanity=\"\"\"\n",
    "WITH pair_features_count AS (\n",
    "    SELECT\n",
    "        COUNT(*) AS total_pair_features\n",
    "    FROM er.pair_features\n",
    "), pair_scores_count AS (\n",
    "    SELECT\n",
    "        COUNT(*) AS total_pair_scores\n",
    "    FROM er.pair_scores\n",
    "), pair_features_scored_count AS (\n",
    "    SELECT\n",
    "        COUNT(*) AS total_pair_features_scored\n",
    "    FROM er.pair_features_scored\n",
    "), null_scores_count AS (\n",
    "    SELECT\n",
    "        COUNT(*) AS total_null_pair_features_scored\n",
    "    FROM er.pair_features_scored\n",
    "    WHERE model_score IS NULL\n",
    ")\n",
    "SELECT\n",
    "    t1.total_pair_features,\n",
    "    t2.total_pair_scores,\n",
    "    t3.total_pair_features_scored,\n",
    "    t4.total_null_pair_features_scored\n",
    "FROM\n",
    "    pair_features_count t1,\n",
    "    pair_scores_count t2,\n",
    "    pair_features_scored_count t3,\n",
    "    null_scores_count t4;\n",
    "\"\"\"\n",
    "with duckdb.connect(database=DB_PATH, read_only=True) as conn:\n",
    "    try:\n",
    "        rep_df = pd.read_sql(better_sanity, conn)\n",
    "    except duckdb.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "print(rep_df.to_csv(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resolver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
